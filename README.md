# ğŸ¥ Cancer-Impact-Predictor
Cancer-Impact-Predictor is a machine learning model designed to predict the average number of cancer-related deaths per year based on various features. This project applies advanced data preprocessing and multiple regression models to analyze historical trends in cancer mortality.

## ğŸ“Š Project Overview
OncoPredict AI aims to provide insights into cancer-related deaths using machine learning models. The project involves:
âœ”ï¸ Handling missing values using mean and median imputation
âœ”ï¸ Encoding categorical features using One-Hot Encoding (OHE) and Label Encoding
âœ”ï¸ Removing irrelevant columns to improve model efficiency
âœ”ï¸ Outlier treatment using IQR Winsorization
âœ”ï¸ Splitting data into 80% training and 20% testing
âœ”ï¸ Applying various regression models to predict mortality trends.

## ğŸ“ Dataset & Preprocessing
* Missing values were replaced with mean and median.
* Categorical variables were processed using One-Hot Encoding (OHE) and Label Encoding.
* Irrelevant columns were removed to improve model accuracy.
* Outliers in numerical columns were treated using IQR Winsorization:
    Lower outliers replaced with the lower fence
    Upper outliers replaced with the upper fence
* Data was split into 80% training and 20% testing.
* Linear Regression assumptions were checked:
    Linearity,
    No autocorrelation,
    Homoscedasticity,
    Normality of errors,
    Multicollinearity.

## ğŸ“ˆ Model Performance
![image](https://github.com/user-attachments/assets/cb9bbbb3-ee9a-44a1-9459-63ea03ae8f77)

## ğŸ“Š Results
Polynomial Regression performed the best with an RÂ² score of 0.992.
Linear Regression and KNN Regression also had high accuracy.
SVR Regression had a lower testing accuracy (0.85), indicating room for improvement.
Decision Tree Regression performed well but may benefit from hyperparameter tuning.

## ğŸ” Conclusion
Polynomial Regression was the most effective model.
Linear and KNN Regression also provided high accuracy.
SVR Regression struggled with generalization.
Further feature selection and model tuning could enhance performance.

## ğŸ”® Future Improvements
Try different feature selection techniques to improve model accuracy.
Optimize hyperparameters using GridSearchCV.
Implement ensemble models like Random Forest or Gradient Boosting.
Deploy the model using Flask or Streamlit for real-world use.

## ğŸ‘¤ Author
Hasti Jadav
ğŸ‘¨â€ğŸ’» Data Science & Machine Learning Enthusiast
ğŸ“§ hastijadav03@gmail.com
ğŸ”— https://www.linkedin.com/in/hasti-jadav-91a210238/



